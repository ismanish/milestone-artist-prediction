{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f622b29b-af93-49b4-86f9-05cfe1cd91ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import textwrap\n",
    "import torchvision.models as models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1cd76cf3-8658-4000-92be-4830b3106106",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([transforms.Resize((128, 128)), \n",
    "                                transforms.ToTensor()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f041b4de-f25a-43fb-947b-e7dad86e088c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# os.listdir('./data/wikiart_sample/New_Realism')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91d2ec39-96f7-4fda-bd23-29ed57238668",
   "metadata": {},
   "outputs": [],
   "source": [
    "images = {}\n",
    "\n",
    "folder_path = './data/wikiart_sample_large/New_Realism'\n",
    "\n",
    "for img_file in os.listdir('./data/wikiart_sample_large/New_Realism'):\n",
    "    img_path = os.path.join(folder_path, img_file)\n",
    "    img = Image.open(img_path).convert('RGB')\n",
    "    img_tensor = transform(img)\n",
    "    img_flat = torch.flatten(img_tensor)\n",
    "    images[img_path] = img_flat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e5e277f-486e-45f2-bd70-278a2d31d5ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pearson_correlation(tensor1, tensor2):\n",
    "    mean1 = torch.mean(tensor1)\n",
    "    mean2 = torch.mean(tensor2)\n",
    "    std1 = torch.std(tensor1)\n",
    "    std2 = torch.std(tensor2)\n",
    "    n = len(tensor1)\n",
    "    \n",
    "    corr = torch.sum((tensor1 - mean1) * (tensor2 - mean2)) / (std1 * std2 * n)\n",
    "    return corr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "150564c5-e94f-4f32-9728-2e8632bc2644",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For storing the most correlated image for each image\n",
    "most_correlated = {}\n",
    "\n",
    "for img1, tensor1 in images.items():\n",
    "    max_corr = float('-inf')\n",
    "    max_corr_img = None\n",
    "    \n",
    "    for img2, tensor2 in images.items():\n",
    "        if img1 != img2:\n",
    "            corr = pearson_correlation(tensor1, tensor2).item()\n",
    "            \n",
    "            if corr > max_corr:\n",
    "                max_corr = corr\n",
    "                max_corr_img = img2\n",
    "                \n",
    "    # most_correlated[img1] = max_corr_img\n",
    "    most_correlated[img1] = (max_corr_img, max_corr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd168528-e1da-4734-9ecd-e66db416845a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('random_keys.txt', 'w') as f:\n",
    "#     for key in random_images:\n",
    "#         f.write(f\"{key}\\n\")\n",
    "        \n",
    "with open('random_keys.txt', 'r') as f:\n",
    "    random_images = [line.strip() for line in f.readlines()]\n",
    "    \n",
    "# random_images = random.sample(list(most_correlated.keys()), 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1af50c56-3b05-478b-a8a5-5ca9d8dc6d30",
   "metadata": {},
   "outputs": [],
   "source": [
    "random_images = [im.replace('_sample','_sample_large') for im in random_images]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abac32fb-9935-431e-8be8-20010bf9871e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a 2x10 plot\n",
    "fig, axes = plt.subplots(2, 10, figsize=(50, 16))\n",
    "\n",
    "# Add labels for the rows\n",
    "axes[0, 0].annotate('Original', xy=(0, 0.5), xytext=(-axes[0,0].yaxis.labelpad - 5, 0),\n",
    "                    xycoords=axes[0, 0].yaxis.label, textcoords='offset points',\n",
    "                    size=28, ha='right', va='center', weight='bold')\n",
    "axes[1, 0].annotate('Most Correlated', xy=(0, 0.5), xytext=(-axes[1,0].yaxis.labelpad - 5, 0),\n",
    "                    xycoords=axes[1, 0].yaxis.label, textcoords='offset points',\n",
    "                    size=28, ha='right', va='center', weight='bold')\n",
    "\n",
    "# Plot the actual and most correlated images\n",
    "for i, img_path in enumerate(random_images):\n",
    "    # Open and show the actual image\n",
    "    img = Image.open(img_path)\n",
    "    axes[0, i].imshow(img)\n",
    "    axes[0, i].set_aspect('auto')\n",
    "    wrapped_title = textwrap.fill(os.path.basename(img_path[:-4]), 20)\n",
    "    axes[0, i].set_title(wrapped_title, fontsize=20, weight = 'bold')\n",
    "    axes[0, i].axis('off')\n",
    "\n",
    "    # Open and show the most correlated image and correlation value\n",
    "    most_corr_img_path, corr_value = most_correlated[img_path]\n",
    "    most_corr_img = Image.open(most_corr_img_path)\n",
    "    axes[1, i].imshow(most_corr_img)\n",
    "    axes[1, i].set_aspect('auto')\n",
    "    wrapped_title = textwrap.fill(os.path.basename(most_corr_img_path)[:-4], 20)\n",
    "    axes[1, i].set_title(f\"{wrapped_title}\\nCorr: {corr_value:.2f}\", fontsize=20, weight = 'bold')\n",
    "    axes[1, i].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "plt.savefig('simple_correlation_similarity.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b19e62fe-b0ea-476c-8e40-4f03457286d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the pre-trained VGG16 model + the higher level layers\n",
    "# vgg16 = models.vgg16(pretrained=True).features.eval() #This extracts only the convolutional features\n",
    "\n",
    "vgg16 = models.vgg16(pretrained=True) #This approach utilizes not just the convolutional base (features) but also the fully connected layers (classifier)\n",
    "\n",
    "# Remove the classification layers\n",
    "new_classifier = torch.nn.Sequential(*list(vgg16.classifier.children())[:-1])\n",
    "vgg16.classifier = new_classifier\n",
    "\n",
    "# Disable training for all layers\n",
    "for param in vgg16.parameters():\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfc008b8-0d98-495f-872f-972ebfe27788",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def extract_features(image, model):\n",
    "#     x = image.unsqueeze(0)\n",
    "#     for layer in model:\n",
    "#         x = layer(x)\n",
    "#         if isinstance(layer, nn.MaxPool2d):\n",
    "#             x = x.view(x.size(0), -1)\n",
    "#             return x\n",
    "\n",
    "def extract_features(img_tensor, model):\n",
    "    x = img_tensor.unsqueeze(0)\n",
    "    features = model(x)\n",
    "    return features.squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67062d05-56b1-48f2-bd33-b6ba02fb3f4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "images_features = {}\n",
    "\n",
    "for img_file in os.listdir('./data/wikiart_sample_large/New_Realism'):\n",
    "    img_path = os.path.join(folder_path, img_file)\n",
    "    img = Image.open(img_path).convert('RGB')\n",
    "    img_tensor = transform(img)\n",
    "    feature_vector = extract_features(img_tensor, vgg16)\n",
    "    images_features[img_path] = feature_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b079b159-ce47-40f5-bdcb-55f1c778c629",
   "metadata": {},
   "outputs": [],
   "source": [
    "most_correlated_features = {}\n",
    "\n",
    "for img1, feature1 in images_features.items():\n",
    "    max_corr = float('-inf')\n",
    "    max_corr_img = None\n",
    "    \n",
    "    for img2, feature2 in images_features.items():\n",
    "        if img1 != img2:\n",
    "            corr = pearson_correlation(feature1, feature2).item()\n",
    "            \n",
    "            if corr > max_corr:\n",
    "                max_corr = corr\n",
    "                max_corr_img = img2\n",
    "                \n",
    "    # most_correlated_features[img1] = max_corr_img\n",
    "    most_correlated_features[img1] = (max_corr_img, max_corr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9d6c27c-0f6c-4dc9-8cde-0e8574550f63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pick 10 random keys (image paths) from the most_correlated dictionary\n",
    "# random_images = random.sample(list(most_correlated_features.keys()), 10)\n",
    "\n",
    "# Create a 2x10 plot\n",
    "fig, axes = plt.subplots(2, 10, figsize=(50, 16))\n",
    "\n",
    "\n",
    "# Add labels for the rows\n",
    "axes[0, 0].annotate('Original', xy=(0, 0.5), xytext=(-axes[0,0].yaxis.labelpad - 5, 0),\n",
    "                    xycoords=axes[0, 0].yaxis.label, textcoords='offset points',\n",
    "                    size=28, ha='right', va='center', weight = 'bold')\n",
    "axes[1, 0].annotate('Most Correlated', xy=(0, 0.5), xytext=(-axes[1,0].yaxis.labelpad - 5, 0),\n",
    "                    xycoords=axes[1, 0].yaxis.label, textcoords='offset points',\n",
    "                    size=28, ha='right', va='center', weight = 'bold')\n",
    "\n",
    "# Plot the actual and most correlated images\n",
    "for i, img_path in enumerate(random_images):\n",
    "    # Open and show the actual image\n",
    "    img = Image.open(img_path)\n",
    "    axes[0, i].imshow(img)\n",
    "    axes[0, i].set_aspect('auto')\n",
    "    wrapped_title = textwrap.fill(os.path.basename(img_path[:-4]), 20)\n",
    "    axes[0, i].set_title(wrapped_title, fontsize=20, weight = 'bold')\n",
    "    axes[0, i].axis('off')\n",
    "    \n",
    "    # Open and show the most correlated image\n",
    "    # most_corr_img_path = most_correlated_features[img_path]\n",
    "    most_corr_img_path, corr_value = most_correlated_features[img_path]\n",
    "    most_corr_img = Image.open(most_corr_img_path)\n",
    "    axes[1, i].imshow(most_corr_img)\n",
    "    axes[1, i].set_aspect('auto')\n",
    "    wrapped_title = textwrap.fill(os.path.basename(most_corr_img_path)[:-4], 20)\n",
    "    # axes[1, i].set_title(wrapped_title, fontsize=20)\n",
    "    axes[1, i].set_title(f\"{wrapped_title}\\nCorr: {corr_value:.2f}\", fontsize=20, weight = 'bold')\n",
    "    axes[1, i].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "# plt.savefig('feature_correlation_similarity.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fd7fce2-b5e9-414d-8a3c-4d2d041ee18f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PyTorch 1.13 (Local)",
   "language": "python",
   "name": "local-pytorch-1-13"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
